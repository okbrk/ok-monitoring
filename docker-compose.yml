services:
  # Grafana - Admin-only access via Tailscale VPN
  grafana:
    image: grafana/grafana:11.0.0
    container_name: grafana
    restart: unless-stopped
    user: "472" # Grafana user ID - fixes permission issues
    ports:
      - "3000:3000" # Exposed on host, but NOT via Caddy
    environment:
      - GF_SERVER_ROOT_URL=http://${TAILSCALE_IP:-localhost}:3000
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=
    volumes:
      - ${DATA_DIR}/grafana:/var/lib/grafana
      - ./config/grafana/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml:ro
      - ./config/grafana/dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml:ro
    networks:
      - observability
    depends_on:
      - loki
      - mimir
      - tempo

  # Loki - Multi-tenant log aggregation
  loki:
    image: grafana/loki:3.0.0
    container_name: loki
    restart: unless-stopped
    user: "10001" # Loki user ID - fixes permission issues
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/loki.yaml
    environment:
      - S3_ENDPOINT=${WASABI_ENDPOINT}
      - S3_REGION=${WASABI_REGION}
      - S3_LOKI_BUCKET=${S3_LOKI_BUCKET}
      - S3_LOKI_ACCESS_KEY_ID=${S3_LOKI_ACCESS_KEY_ID}
      - S3_LOKI_SECRET_ACCESS_KEY=${S3_LOKI_SECRET_ACCESS_KEY}
    volumes:
      - ./config/loki/loki.yaml:/etc/loki/loki.yaml:ro
      - ${DATA_DIR}/loki:/loki
    networks:
      - observability

  # Mimir - Multi-tenant metrics storage
  mimir:
    image: grafana/mimir:2.12.0
    container_name: mimir
    restart: unless-stopped
    ports:
      - "8080:8080"
    command:
      - -config.file=/etc/mimir/mimir.yaml
      - -target=all
    environment:
      - S3_ENDPOINT=${WASABI_ENDPOINT}
      - S3_REGION=${WASABI_REGION}
      - S3_MIMIR_BUCKET=${S3_MIMIR_BUCKET}
      - S3_MIMIR_ACCESS_KEY_ID=${S3_MIMIR_ACCESS_KEY_ID}
      - S3_MIMIR_SECRET_ACCESS_KEY=${S3_MIMIR_SECRET_ACCESS_KEY}
    volumes:
      - ./config/mimir/mimir.yaml:/etc/mimir/mimir.yaml:ro
      - ${DATA_DIR}/mimir:/data
    networks:
      - observability

  # Tempo - Multi-tenant distributed tracing
  tempo:
    image: grafana/tempo:2.4.0
    container_name: tempo
    restart: unless-stopped
    ports:
      - "3200:3200" # Tempo HTTP
      - "4317" # OTLP gRPC
      - "4318" # OTLP HTTP
    command:
      - -config.file=/etc/tempo/tempo.yaml
    environment:
      - S3_ENDPOINT=${WASABI_ENDPOINT}
      - S3_REGION=${WASABI_REGION}
      - S3_TEMPO_BUCKET=${S3_TEMPO_BUCKET}
      - S3_TEMPO_ACCESS_KEY_ID=${S3_TEMPO_ACCESS_KEY_ID}
      - S3_TEMPO_SECRET_ACCESS_KEY=${S3_TEMPO_SECRET_ACCESS_KEY}
    volumes:
      - ./config/tempo/tempo.yaml:/etc/tempo/tempo.yaml:ro
      - ${DATA_DIR}/tempo:/var/tempo
    networks:
      - observability

  # OpenTelemetry Collector - Gateway for customer data
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.100.0
    container_name: otel-collector
    restart: unless-stopped
    ports:
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
      - "8888:8888" # Prometheus metrics
    command:
      - --config=/etc/otel-collector/config.yaml
    volumes:
      - ./config/otel-collector/config.yaml:/etc/otel-collector/config.yaml:ro
    networks:
      - observability
    depends_on:
      - loki
      - mimir
      - tempo

  # Caddy - Reverse proxy with automatic HTTPS
  caddy:
    image: caddy:2.7-alpine
    container_name: caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp" # HTTP/3
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ${DATA_DIR}/caddy/data:/data
      - ${DATA_DIR}/caddy/config:/config
    networks:
      - observability
    depends_on:
      - grafana
      - loki
      - mimir
      - tempo
      - otel-collector
    environment:
      - DOMAIN=${DOMAIN}

  # PostgreSQL - Tenant management database
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=tenants
      - POSTGRES_USER=tenants
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - ${DATA_DIR}/postgres:/var/lib/postgresql/data
      - ./config/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks:
      - observability
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U tenants"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Grafana Agent - Scrapes customer Prometheus metrics endpoints
  grafana-agent:
    image: grafana/agent:v0.40.0
    container_name: grafana-agent
    restart: unless-stopped
    ports:
      - "12345:12345"
    command:
      - run
      - /etc/agent/config.yaml
    environment:
      - AGENT_MODE=flow
    volumes:
      - ./config/grafana-agent/config.yaml:/etc/agent/config.yaml:ro
      - ${DATA_DIR}/grafana-agent:/tmp/agent
    networks:
      - observability
    depends_on:
      - mimir
      - postgres

  # Prometheus - Infrastructure metrics collection
  prometheus:
    image: prom/prometheus:v2.51.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - ${DATA_DIR}/prometheus:/prometheus
    networks:
      - observability
    depends_on:
      - mimir
      - alertmanager

  # Alertmanager - Alert routing and notifications
  alertmanager:
    image: prom/alertmanager:v0.27.0
    container_name: alertmanager
    restart: unless-stopped
    ports:
      - "9093:9093"
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    volumes:
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - ${DATA_DIR}/alertmanager:/alertmanager
    networks:
      - observability

  # Node Exporter - Host-level metrics
  node-exporter:
    image: prom/node-exporter:v1.7.0
    container_name: node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    command:
      - "--path.rootfs=/host"
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    volumes:
      - /:/host:ro,rslave
    networks:
      - observability
    pid: host

  # cAdvisor - Container-level metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8081:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    devices:
      - /dev/kmsg
    networks:
      - observability

  # PostgreSQL Exporter - Database metrics
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    restart: unless-stopped
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://tenants:${POSTGRES_PASSWORD}@postgres:5432/tenants?sslmode=disable
    networks:
      - observability
    depends_on:
      - postgres

networks:
  observability:
    driver: bridge
