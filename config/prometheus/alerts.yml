groups:
  # Service Health Alerts
  - name: service_health
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "Service {{ $labels.service }} is down"
          description: "{{ $labels.service }} has been down for more than 2 minutes."

      - alert: ServiceFlapping
        expr: changes(up[10m]) > 5
        for: 5m
        labels:
          severity: warning
          category: availability
        annotations:
          summary: "Service {{ $labels.service }} is flapping"
          description: "{{ $labels.service }} has restarted {{ $value }} times in the last 10 minutes."

  # Resource Usage Alerts - Host Level
  - name: host_resources
    interval: 30s
    rules:
      - alert: HostHighCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 90
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 90%)."

      - alert: HostCriticalCPU
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
          category: performance
        annotations:
          summary: "Critical CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 95%)."

      - alert: HostHighMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 80%)."

      - alert: HostCriticalMemory
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
          category: performance
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 90%)."

      - alert: HostHighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"} / node_filesystem_size_bytes)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          category: capacity
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk {{ $labels.mountpoint }} is {{ $value | humanize }}% full (threshold: 80%)."

      - alert: HostCriticalDiskUsage
        expr: (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"} / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
          category: capacity
        annotations:
          summary: "Critical disk usage on {{ $labels.instance }}"
          description: "Disk {{ $labels.mountpoint }} is {{ $value | humanize }}% full (threshold: 90%). Immediate action required!"

      - alert: DiskWillFillIn24Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs|squashfs|vfat"}[6h], 24*3600) < 0
        for: 30m
        labels:
          severity: warning
          category: capacity
        annotations:
          summary: "Disk {{ $labels.mountpoint }} will fill in 24 hours"
          description: "Based on current growth rate, disk {{ $labels.mountpoint }} on {{ $labels.instance }} will fill in approximately 24 hours."

  # Container Resource Alerts
  - name: container_resources
    interval: 30s
    rules:
      - alert: ContainerHighCPU
        expr: sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) by (name) * 100 > 80
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Container {{ $labels.name }} high CPU usage"
          description: "Container {{ $labels.name }} CPU usage is {{ $value | humanize }}%."

      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""}) * 100 > 80
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "Container {{ $labels.name }} high memory usage"
          description: "Container {{ $labels.name }} memory usage is {{ $value | humanize }}% of limit."

      - alert: ContainerRestarting
        expr: rate(container_last_seen{name!=""}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          category: availability
        annotations:
          summary: "Container {{ $labels.name }} is restarting frequently"
          description: "Container {{ $labels.name }} has restarted frequently in the last 5 minutes."

  # Loki Specific Alerts
  - name: loki_alerts
    interval: 30s
    rules:
      - alert: LokiIngestionErrors
        expr: rate(loki_ingester_errors_total[5m]) > 0.01
        for: 5m
        labels:
          severity: warning
          category: ingestion
        annotations:
          summary: "Loki ingestion errors detected"
          description: "Loki is experiencing {{ $value | humanize }} errors/sec during ingestion."

      - alert: LokiHighIngestionRate
        expr: sum(rate(loki_distributor_lines_received_total[5m])) > 10000
        for: 10m
        labels:
          severity: info
          category: ingestion
        annotations:
          summary: "Loki high ingestion rate"
          description: "Loki is ingesting {{ $value | humanize }} lines/sec (monitoring for capacity planning)."

  # Mimir Specific Alerts
  - name: mimir_alerts
    interval: 30s
    rules:
      - alert: MimirIngestionErrors
        expr: rate(cortex_discarded_samples_total[5m]) > 0.01
        for: 5m
        labels:
          severity: warning
          category: ingestion
        annotations:
          summary: "Mimir discarding samples"
          description: "Mimir is discarding {{ $value | humanize }} samples/sec. Check for timestamp or limit issues."

      - alert: MimirHighIngestionRate
        expr: sum(rate(cortex_distributor_received_samples_total[5m])) > 50000
        for: 10m
        labels:
          severity: info
          category: ingestion
        annotations:
          summary: "Mimir high ingestion rate"
          description: "Mimir is ingesting {{ $value | humanize }} samples/sec (monitoring for capacity planning)."

      - alert: MimirCompactionFailing
        expr: increase(cortex_compactor_runs_failed_total[1h]) > 0
        for: 30m
        labels:
          severity: warning
          category: storage
        annotations:
          summary: "Mimir compaction failures"
          description: "Mimir compactor has failed {{ $value }} times in the last hour."

  # Tempo Specific Alerts
  - name: tempo_alerts
    interval: 30s
    rules:
      - alert: TempoIngestionErrors
        expr: rate(tempo_distributor_spans_received_total[5m]) - rate(tempo_ingester_spans_received_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
          category: ingestion
        annotations:
          summary: "Tempo ingestion errors detected"
          description: "Tempo is losing spans between distributor and ingester."

  # PostgreSQL Alerts
  - name: postgres_alerts
    interval: 30s
    rules:
      - alert: PostgresTooManyConnections
        expr: sum(pg_stat_activity_count) by (instance) > 80
        for: 5m
        labels:
          severity: warning
          category: database
        annotations:
          summary: "PostgreSQL has too many connections"
          description: "PostgreSQL instance {{ $labels.instance }} has {{ $value }} connections (monitor for connection leaks)."

      - alert: PostgresDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
          category: availability
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL exporter cannot connect to the database."

      - alert: PostgresSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 60
        for: 10m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "PostgreSQL has slow queries"
          description: "PostgreSQL has queries running for more than 60 seconds."

  # OTEL Collector Alerts
  - name: otel_alerts
    interval: 30s
    rules:
      - alert: OTELHighDropRate
        expr: rate(otelcol_processor_dropped_spans[5m]) > 10
        for: 5m
        labels:
          severity: warning
          category: ingestion
        annotations:
          summary: "OTEL Collector dropping spans"
          description: "OTEL Collector is dropping {{ $value | humanize }} spans/sec."

      - alert: OTELHighMemoryUsage
        expr: otelcol_process_memory_rss / 1024 / 1024 > 400
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "OTEL Collector high memory usage"
          description: "OTEL Collector is using {{ $value | humanize }}MB of memory (limit: 512MB in config)."

  # Alertmanager Health
  - name: alertmanager_alerts
    interval: 30s
    rules:
      - alert: AlertmanagerFailedNotifications
        expr: rate(alertmanager_notifications_failed_total[5m]) > 0.01
        for: 10m
        labels:
          severity: warning
          category: alerting
        annotations:
          summary: "Alertmanager notification failures"
          description: "Alertmanager is failing to send notifications at {{ $value | humanize }} failures/sec."

      - alert: AlertmanagerConfigReloadFailed
        expr: alertmanager_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
          category: alerting
        annotations:
          summary: "Alertmanager config reload failed"
          description: "Alertmanager failed to reload its configuration."

